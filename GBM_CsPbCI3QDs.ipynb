{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The following codes show the performance of the test and train data using GBM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Gradient Boosting Machine (GBM) Overview**\n",
    "\n",
    "Gradient Boosting Machine (GBM) is a powerful ensemble learning technique commonly used for regression and classification tasks. It builds predictive models by sequentially combining the outputs of weaker learners (typically decision trees) to improve overall accuracy.\n",
    "\n",
    "In this project, GBM is used to predict the target variables (`size_nm`, `S_abs_nm_Y1`, and `PL`) by learning from the provided dataset. The key concepts behind GBM include:\n",
    "\n",
    "1. **Boosting**: \n",
    "   - Boosting is an iterative technique where each subsequent model corrects the errors of its predecessor.\n",
    "   - GBM builds models in a stage-wise fashion, optimizing the loss function at each step.\n",
    "\n",
    "2. **Gradient Descent**:\n",
    "   - GBM minimizes the loss function (e.g., Mean Squared Error for regression) by using gradient descent to update predictions iteratively.\n",
    "\n",
    "3. **Hyperparameters**:\n",
    "   - **Number of Trees (`n_estimators`)**: Determines the number of weak learners to be combined.\n",
    "   - **Learning Rate (`learning_rate`)**: Controls the contribution of each tree to the overall model, balancing between underfitting and overfitting.\n",
    "   - **Maximum Features (`max_features`)**: Limits the number of features considered at each split for reducing complexity.\n",
    "\n",
    "4. **Advantages**:\n",
    "   - Handles complex, non-linear relationships effectively.\n",
    "   - Robust to missing data and noisy datasets.\n",
    "\n",
    "5. **Limitations**:\n",
    "   - Can be prone to overfitting if hyperparameters are not tuned carefully.\n",
    "   - Training can be computationally expensive for large datasets.\n",
    "\n",
    "In this analysis, GBM is optimized using **Grid Search** with a set of hyperparameters to find the best model. Model performance is evaluated using metrics such as:\n",
    "- **RÂ² (Coefficient of Determination)**: Measures how well the model explains the variance in the target variable.\n",
    "- **RMSE (Root Mean Squared Error)**: Quantifies the average magnitude of errors in predictions.\n",
    "- **MAE (Mean Absolute Error)**: Captures the average absolute difference between observed and predicted values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV, RepeatedKFold, train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# File Paths\n",
    "file_path_original = \"./CsPbCl3_QDs.xlsx\"  # Original dataset\n",
    "file_path_modified = \"./modified_data.xlsx\"  # Preprocessed dataset\n",
    "\n",
    "# Step 1: Load and Preprocess Data\n",
    "def load_and_preprocess_data(file_path):\n",
    "    \"\"\"\n",
    "    Load and preprocess the dataset.\n",
    "    \n",
    "    Parameters:\n",
    "        file_path (str): Path to the Excel file.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Preprocessed dataset.\n",
    "    \"\"\"\n",
    "    data = pd.read_excel(file_path)\n",
    "    \n",
    "    # Identify categorical columns\n",
    "    categorical_columns = data.select_dtypes(include=['object']).columns\n",
    "    \n",
    "    # Apply one-hot encoding to categorical columns\n",
    "    one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
    "    one_hot_encoded = one_hot_encoder.fit_transform(data[categorical_columns])\n",
    "    one_hot_encoded_df = pd.DataFrame(\n",
    "        one_hot_encoded, \n",
    "        columns=one_hot_encoder.get_feature_names_out(categorical_columns)\n",
    "    )\n",
    "    \n",
    "    # Replace categorical columns with one-hot encoded columns\n",
    "    data_encoded = data.drop(categorical_columns, axis=1)\n",
    "    data_encoded = pd.concat([data_encoded, one_hot_encoded_df], axis=1)\n",
    "    return data_encoded\n",
    "\n",
    "# Load datasets\n",
    "data_original = load_and_preprocess_data(file_path_original)\n",
    "data_modified = load_and_preprocess_data(file_path_modified)\n",
    "\n",
    "# Step 2: Prepare Data for Machine Learning\n",
    "def prepare_ml_data(data, target_column):\n",
    "    \"\"\"\n",
    "    Prepare the dataset for machine learning.\n",
    "    \n",
    "    Parameters:\n",
    "        data (pd.DataFrame): Dataset.\n",
    "        target_column (str): Target variable.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple: Features (X), target (y), and train-test splits (X_train, X_test, y_train, y_test).\n",
    "    \"\"\"\n",
    "    X = data.drop(target_column, axis=1)\n",
    "    y = data[target_column]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Step 3: Train Gradient Boosting Model\n",
    "def train_gradient_boosting(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Train a Gradient Boosting Regressor on the dataset.\n",
    "    \n",
    "    Parameters:\n",
    "        X_train (pd.DataFrame): Training features.\n",
    "        y_train (pd.Series): Training target.\n",
    "        X_test (pd.DataFrame): Testing features.\n",
    "        y_test (pd.Series): Testing target.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Model predictions and performance metrics.\n",
    "    \"\"\"\n",
    "    # Fill missing values\n",
    "    X_train_filled = X_train.fillna(X_train.mean())\n",
    "    X_test_filled = X_test.fillna(X_train.mean())\n",
    "    y_train_filled = y_train.fillna(y_train.mean())\n",
    "    \n",
    "    # Define parameter grid\n",
    "    param_grid = {\n",
    "        'max_features': ['sqrt', 'log2'],\n",
    "        'n_estimators': [100, 150, 200],\n",
    "        'learning_rate': [0.05, 0.1, 0.15]\n",
    "    }\n",
    "    gbm = GradientBoostingRegressor(random_state=42)\n",
    "    grid_search = GridSearchCV(gbm, param_grid, cv=RepeatedKFold(n_splits=5, n_repeats=3), scoring='r2', verbose=1)\n",
    "    grid_search.fit(X_train_filled, y_train_filled)\n",
    "    \n",
    "    # Predictions\n",
    "    predictions_train = grid_search.predict(X_train_filled)\n",
    "    predictions_test = grid_search.predict(X_test_filled)\n",
    "    \n",
    "    # Performance metrics\n",
    "    metrics = {\n",
    "        \"Train R2\": r2_score(y_train_filled, predictions_train),\n",
    "        \"Train RMSE\": np.sqrt(mean_squared_error(y_train_filled, predictions_train)),\n",
    "        \"Train MAE\": mean_absolute_error(y_train_filled, predictions_train),\n",
    "        \"Test R2\": r2_score(y_test, predictions_test),\n",
    "        \"Test RMSE\": np.sqrt(mean_squared_error(y_test, predictions_test)),\n",
    "        \"Test MAE\": mean_absolute_error(y_test, predictions_test)\n",
    "    }\n",
    "    return {\n",
    "        \"predictions_train\": predictions_train,\n",
    "        \"predictions_test\": predictions_test,\n",
    "        \"metrics\": metrics\n",
    "    }\n",
    "\n",
    "# Step 4: Evaluate Targets\n",
    "targets = ['size_nm', 'S_abs_nm_Y1', 'PL']\n",
    "results = {}\n",
    "\n",
    "for target in targets:\n",
    "    print(f\"Evaluating target: {target}\")\n",
    "    X_train, X_test, y_train, y_test = prepare_ml_data(data_modified, target)\n",
    "    results[target] = train_gradient_boosting(X_train, y_train, X_test, y_test)\n",
    "\n",
    "    # Print metrics\n",
    "    print(f\"Metrics for {target}:\")\n",
    "    for metric, value in results[target][\"metrics\"].items():\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Step 5: Visualization\n",
    "fig, axs = plt.subplots(3, 2, figsize=(15, 15))\n",
    "\n",
    "for i, target in enumerate(targets):\n",
    "    y_test = results[target]['predictions_test']\n",
    "    predictions_test = results[target]['predictions_test']\n",
    "    \n",
    "    # Plot 1: Observed vs Predicted\n",
    "    sns.scatterplot(x=np.arange(len(y_test)), y=y_test, ax=axs[i, 0], label='Observed', color='red')\n",
    "    sns.scatterplot(x=np.arange(len(predictions_test)), y=predictions_test, ax=axs[i, 0], label='Predicted', color='blue')\n",
    "    axs[i, 0].set_title(f'{target} - Observed vs Predicted')\n",
    "    \n",
    "    # Plot 2: Residuals\n",
    "    residuals = y_test - predictions_test\n",
    "    sns.histplot(residuals, ax=axs[i, 1], kde=True, color='green')\n",
    "    axs[i, 1].set_title(f'{target} - Residuals Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
