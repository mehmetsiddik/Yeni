{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Regression of the all  Outputs of the CsPbCI3QDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# **Introduction**\n",
    "\n",
    "This analysis focuses on modeling and predicting the properties of CsPbCl3 quantum dots using Support Vector Regression (SVR). The workflow includes data preprocessing, feature engineering, model training, evaluation, and result visualization.\n",
    "\n",
    "---\n",
    "\n",
    "# **1. Data Loading and Preprocessing**\n",
    "\n",
    "The dataset is loaded from two sources:\n",
    "- **Original Dataset**: `CsPbCl3_QDs.xlsx`\n",
    "- **Modified Dataset**: `modified_data.xlsx`\n",
    "\n",
    "### Key Preprocessing Steps:\n",
    "- **One-Hot Encoding**: Converts categorical variables into numerical format.\n",
    "- **Scaling**: Standardizes numerical features for better performance with SVR.\n",
    "- **PCA (Principal Component Analysis)**: Reduces dimensionality while preserving variance.\n",
    "\n",
    "---\n",
    "\n",
    "# **2. Model Training and Evaluation**\n",
    "\n",
    "### Support Vector Regression (SVR):\n",
    "SVR is a powerful regression method that fits a hyperplane to predict continuous values while minimizing errors. The `rbf` kernel is used for its ability to model non-linear relationships.\n",
    "\n",
    "### Evaluation Metrics:\n",
    "- **RÂ² (Coefficient of Determination)**: Measures the proportion of variance explained by the model.\n",
    "- **RMSE (Root Mean Squared Error)**: Quantifies the average prediction error in the same units as the target variable.\n",
    "- **MAE (Mean Absolute Error)**: Captures the average magnitude of errors.\n",
    "\n",
    "### Hyperparameter Tuning:\n",
    "GridSearchCV is used to optimize the SVR hyperparameters:\n",
    "- `C`: Regularization parameter controlling model complexity.\n",
    "- `gamma`: Defines the influence of single training samples.\n",
    "- `epsilon`: Specifies the margin of tolerance for errors.\n",
    "\n",
    "---\n",
    "\n",
    "# **3. Results Visualization**\n",
    "\n",
    "### **Observed vs Predicted Values**\n",
    "Scatter plots compare observed and predicted values for each target variable (`size_nm`, `S_abs_nm_Y1`, `PL`).\n",
    "\n",
    "### **Residual Analysis**\n",
    "Residual histograms visualize the error distribution, helping identify biases or patterns.\n",
    "\n",
    "### **Feature Importance**\n",
    "Although SVR does not provide feature importances directly, external tools like SHAP or permutation importance can estimate the relative contribution of each feature.\n",
    "\n",
    "---\n",
    "\n",
    "# **4. Summary**\n",
    "\n",
    "This analysis highlights the potential of SVR for predicting key properties of CsPbCl3 quantum dots. The model effectively captures complex relationships between input features and target variables. The evaluation metrics and visualizations provide insights into the model's performance.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Regression Analysis for CsPbCl3 Quantum Dots\n",
    "### 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths for datasets\n",
    "file_path_original = \"./CsPbCl3_QDs.xlsx\"\n",
    "file_path_modified = \"./modified_data.xlsx\"\n",
    "\n",
    "# Load the dataset\n",
    "CsPbCl3 = pd.read_excel(file_path_modified)\n",
    "\n",
    "# Identify categorical columns and apply one-hot encoding\n",
    "categorical_columns = CsPbCl3.select_dtypes(include=['object']).columns\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
    "one_hot_encoded = one_hot_encoder.fit_transform(CsPbCl3[categorical_columns])\n",
    "one_hot_encoded_df = pd.DataFrame(\n",
    "    one_hot_encoded, \n",
    "    columns=one_hot_encoder.get_feature_names_out(categorical_columns)\n",
    ")\n",
    "\n",
    "# Replace categorical columns with one-hot encoded columns\n",
    "CsPbCl3_encoded = CsPbCl3.drop(categorical_columns, axis=1).join(one_hot_encoded_df)\n",
    "\n",
    "# Define the target variables\n",
    "targets = ['size_nm', 'S_abs_nm_Y1', 'PL']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train SVR Model and Evaluate Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store results and predictions\n",
    "results = {}\n",
    "predictions = {}\n",
    "\n",
    "# Train SVR model for each target\n",
    "for target in targets:\n",
    "    print(f\"Training SVR for target: {target}\")\n",
    "    \n",
    "    # Prepare features and target variable\n",
    "    X = CsPbCl3_encoded.drop(columns=targets)\n",
    "    y = CsPbCl3_encoded[target]\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Define a pipeline with scaling, PCA, and SVR\n",
    "    pipe = make_pipeline(StandardScaler(), PCA(n_components=0.95), SVR(kernel='rbf'))\n",
    "    \n",
    "    # Define the parameter grid for GridSearchCV\n",
    "    param_grid = {\n",
    "        'svr__C': [0.1, 1, 10, 100],\n",
    "        'svr__gamma': ['scale', 0.01, 0.1, 1],\n",
    "        'svr__epsilon': [0.01, 0.1, 0.5]\n",
    "    }\n",
    "    \n",
    "    # Perform grid search with cross-validation\n",
    "    grid = GridSearchCV(pipe, param_grid, cv=5, scoring='r2', verbose=2)\n",
    "    grid.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"Best parameters for {target}: {grid.best_params_}\")\n",
    "    \n",
    "    # Evaluate the best model\n",
    "    best_model = grid.best_estimator_\n",
    "    predictions_train = best_model.predict(X_train)\n",
    "    predictions_test = best_model.predict(X_test)\n",
    "    \n",
    "    # Store results and predictions\n",
    "    results[target] = {\n",
    "        'Train R2': r2_score(y_train, predictions_train),\n",
    "        'Test R2': r2_score(y_test, predictions_test),\n",
    "        'Train RMSE': np.sqrt(mean_squared_error(y_train, predictions_train)),\n",
    "        'Test RMSE': np.sqrt(mean_squared_error(y_test, predictions_test)),\n",
    "        'Train MAE': mean_absolute_error(y_train, predictions_train),\n",
    "        'Test MAE': mean_absolute_error(y_test, predictions_test)\n",
    "    }\n",
    "    predictions[target] = {'y_test': y_test, 'predictions_test': predictions_test}\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Performance for {target}:\\n\", results[target], \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize observed vs predicted values and residuals\n",
    "fig, axs = plt.subplots(len(targets), 2, figsize=(12, 10))\n",
    "titles = ['Size', '1 S abs', 'PL']\n",
    "\n",
    "for i, target in enumerate(targets):\n",
    "    y_test = predictions[target]['y_test']\n",
    "    predictions_test = predictions[target]['predictions_test']\n",
    "    \n",
    "    # Plot (a): Observed vs Predicted Values\n",
    "    sns.scatterplot(x=np.arange(len(y_test)), y=y_test, ax=axs[i, 0], label='Observed', color='red')\n",
    "    sns.scatterplot(x=np.arange(len(predictions_test)), y=predictions_test, ax=axs[i, 0], label='Predicted', color='blue')\n",
    "    axs[i, 0].set_title(f'{titles[i]}: Observed vs Predicted')\n",
    "    axs[i, 0].set_xlabel('Sample Number')\n",
    "    axs[i, 0].set_ylabel('Values')\n",
    "    axs[i, 0].legend()\n",
    "    \n",
    "    # Plot (b): Residuals\n",
    "    residuals = y_test - predictions_test\n",
    "    sns.histplot(residuals, ax=axs[i, 1], kde=True, color='green')\n",
    "    axs[i, 1].set_title(f'{titles[i]}: Residual Distribution')\n",
    "    axs[i, 1].set_xlabel('Residuals')\n",
    "    axs[i, 1].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Feature Importances (Placeholder for Explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance for SVR requires external libraries like SHAP or permutation importance\n",
    "# Here, we include a placeholder for feature importances\n",
    "\n",
    "# Example predefined feature importances for 'size_nm'\n",
    "feature_importances_size_nm = {\n",
    "    'Pb amount': 0.14,\n",
    "    'Cs/Pb ratio': 0.13,\n",
    "    'Temperature': 0.12,\n",
    "    'ODE volume': 0.11,\n",
    "    'Cs amount': 0.10,\n",
    "    'Pb/(OA + OL)': 0.09,\n",
    "    'Cl/Pb ratio': 0.08\n",
    "}\n",
    "\n",
    "# Create a DataFrame for feature importances\n",
    "feature_importances_df = pd.DataFrame(\n",
    "    list(feature_importances_size_nm.items()), \n",
    "    columns=['Feature', 'Importance']\n",
    ")\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importances_df, palette='coolwarm')\n",
    "plt.title('Feature Importances for Size (Placeholder)')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
