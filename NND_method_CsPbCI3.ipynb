{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NND Method of the  of CsPbCI3 Codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearest Neighbour Distance (NND) Overview\n",
    "\n",
    "Nearest Neighbour Distance (NND) is a technique used in data analysis to evaluate relationships between data points based on their proximity in the feature space. In machine learning, NND is commonly implemented using **K-Nearest Neighbors (KNN)** regression to predict target variables.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Instance-Based Learning**:\n",
    "   - NND relies on the training data to make predictions without explicitly building a model.\n",
    "   - It is referred to as a \"lazy learner\" because computation happens during prediction.\n",
    "\n",
    "2. **Similarity Measure**:\n",
    "   - The method calculates the distance (e.g., Euclidean, Manhattan) between data points to identify the closest neighbors.\n",
    "\n",
    "3. **Hyperparameters**:\n",
    "   - **Number of Neighbors (`k`)**: Determines the number of closest neighbors used for predictions.\n",
    "   - **Distance Metric**: Defines how distances are calculated (e.g., Euclidean, Minkowski).\n",
    "\n",
    "4. **Advantages**:\n",
    "   - Simple and interpretable approach.\n",
    "   - Effective for datasets with localized patterns.\n",
    "\n",
    "5. **Limitations**:\n",
    "   - Computationally expensive for large datasets.\n",
    "   - Sensitive to the choice of `k` and feature scaling.\n",
    "\n",
    "### NND in This Project\n",
    "\n",
    "In this analysis, the **K-Nearest Neighbors (KNN)** regression algorithm is used to predict the target variables (`size_nm`, `S_abs_nm_Y1`, and `PL`). The algorithm works as follows:\n",
    "- The dataset is preprocessed to handle categorical and numerical features.\n",
    "- Feature scaling is applied to ensure distances are computed correctly.\n",
    "- The optimal number of neighbors (`k`) is set to `5` for this implementation.\n",
    "\n",
    "### Evaluation Metrics\n",
    "\n",
    "The model performance is evaluated using the following metrics:\n",
    "- **RÂ² (Coefficient of Determination)**: Measures how well the model explains the variance in the target variable.\n",
    "- **RMSE (Root Mean Squared Error)**: Quantifies the average magnitude of errors in predictions.\n",
    "- **MAE (Mean Absolute Error)**: Captures the average absolute difference between observed and predicted values.\n",
    "\n",
    "### Visualization\n",
    "\n",
    "The results are visualized using:\n",
    "1. Scatter plots to compare observed vs. predicted values.\n",
    "2. Residual plots to assess the distribution of prediction errors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# File Paths\n",
    "file_path_original = \"./CsPbCl3_QDs.xlsx\"\n",
    "file_path_modified = \"./modified_data.xlsx\"\n",
    "\n",
    "# Step 1: Load and Preprocess Data\n",
    "def load_and_preprocess_data(file_path):\n",
    "    \"\"\"\n",
    "    Load and preprocess the dataset.\n",
    "    \n",
    "    Parameters:\n",
    "        file_path (str): Path to the Excel file.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Preprocessed dataset.\n",
    "    \"\"\"\n",
    "    data = pd.read_excel(file_path)\n",
    "    \n",
    "    # Identify categorical columns\n",
    "    categorical_columns = data.select_dtypes(include=['object']).columns\n",
    "    \n",
    "    # Apply one-hot encoding to categorical columns\n",
    "    one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
    "    one_hot_encoded = one_hot_encoder.fit_transform(data[categorical_columns])\n",
    "    one_hot_encoded_df = pd.DataFrame(\n",
    "        one_hot_encoded, \n",
    "        columns=one_hot_encoder.get_feature_names_out(categorical_columns)\n",
    "    )\n",
    "    \n",
    "    # Replace categorical columns with one-hot encoded columns\n",
    "    data_encoded = data.drop(categorical_columns, axis=1)\n",
    "    data_encoded = pd.concat([data_encoded, one_hot_encoded_df], axis=1)\n",
    "    return data_encoded\n",
    "\n",
    "# Load datasets\n",
    "data_original = load_and_preprocess_data(file_path_original)\n",
    "data_modified = load_and_preprocess_data(file_path_modified)\n",
    "\n",
    "# Step 2: Prepare Data for Machine Learning\n",
    "def prepare_ml_data(data, target_column):\n",
    "    \"\"\"\n",
    "    Prepare the dataset for machine learning.\n",
    "    \n",
    "    Parameters:\n",
    "        data (pd.DataFrame): Dataset.\n",
    "        target_column (str): Target variable.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple: Features (X), target (y), and train-test splits (X_train, X_test, y_train, y_test).\n",
    "    \"\"\"\n",
    "    X = data.drop(target_column, axis=1)\n",
    "    y = data[target_column]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Step 3: Train Nearest Neighbour Distance Model\n",
    "def train_nnd_model(X_train, y_train, X_test, y_test, n_neighbors=5):\n",
    "    \"\"\"\n",
    "    Train a KNN regressor on the dataset.\n",
    "    \n",
    "    Parameters:\n",
    "        X_train (pd.DataFrame): Training features.\n",
    "        y_train (pd.Series): Training target.\n",
    "        X_test (pd.DataFrame): Testing features.\n",
    "        y_test (pd.Series): Testing target.\n",
    "        n_neighbors (int): Number of nearest neighbors.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Model predictions and performance metrics.\n",
    "    \"\"\"\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train.fillna(X_train.mean()))\n",
    "    X_test_scaled = scaler.transform(X_test.fillna(X_train.mean()))\n",
    "    \n",
    "    # Define the KNN regressor\n",
    "    knn = KNeighborsRegressor(n_neighbors=n_neighbors)\n",
    "    knn.fit(X_train_scaled, y_train.fillna(y_train.mean()))\n",
    "    \n",
    "    # Predictions\n",
    "    predictions_train = knn.predict(X_train_scaled)\n",
    "    predictions_test = knn.predict(X_test_scaled)\n",
    "    \n",
    "    # Performance metrics\n",
    "    metrics = {\n",
    "        \"Train R2\": r2_score(y_train.fillna(y_train.mean()), predictions_train),\n",
    "        \"Train RMSE\": np.sqrt(mean_squared_error(y_train.fillna(y_train.mean()), predictions_train)),\n",
    "        \"Train MAE\": mean_absolute_error(y_train.fillna(y_train.mean()), predictions_train),\n",
    "        \"Test R2\": r2_score(y_test, predictions_test),\n",
    "        \"Test RMSE\": np.sqrt(mean_squared_error(y_test, predictions_test)),\n",
    "        \"Test MAE\": mean_absolute_error(y_test, predictions_test)\n",
    "    }\n",
    "    return {\n",
    "        \"predictions_train\": predictions_train,\n",
    "        \"predictions_test\": predictions_test,\n",
    "        \"metrics\": metrics\n",
    "    }\n",
    "\n",
    "# Step 4: Evaluate Targets\n",
    "targets = ['size_nm', 'S_abs_nm_Y1', 'PL']\n",
    "results = {}\n",
    "\n",
    "for target in targets:\n",
    "    print(f\"Evaluating target: {target}\")\n",
    "    X_train, X_test, y_train, y_test = prepare_ml_data(data_modified, target)\n",
    "    results[target] = train_nnd_model(X_train, y_train, X_test, y_test, n_neighbors=5)\n",
    "\n",
    "    # Print metrics\n",
    "    print(f\"Metrics for {target}:\")\n",
    "    for metric, value in results[target][\"metrics\"].items():\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Step 5: Visualization\n",
    "fig, axs = plt.subplots(3, 2, figsize=(15, 15))\n",
    "\n",
    "for i, target in enumerate(targets):\n",
    "    y_test = results[target]['predictions_test']\n",
    "    predictions_test = results[target]['predictions_test']\n",
    "    \n",
    "    # Plot 1: Observed vs Predicted\n",
    "    sns.scatterplot(x=np.arange(len(y_test)), y=y_test, ax=axs[i, 0], label='Observed', color='red')\n",
    "    sns.scatterplot(x=np.arange(len(predictions_test)), y=predictions_test, ax=axs[i, 0], label='Predicted', color='blue')\n",
    "    axs[i, 0].set_title(f'{target} - Observed vs Predicted')\n",
    "    \n",
    "    # Plot 2: Residuals\n",
    "    residuals = y_test - predictions_test\n",
    "    sns.histplot(residuals, ax=axs[i, 1], kde=True, color='green')\n",
    "    axs[i, 1].set_title(f'{target} - Residuals Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
