{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning of the all  Outputs of the CsPbCI3QDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Deep Learning Overview**\n",
    "\n",
    "Deep learning is a powerful subset of machine learning that mimics the structure and function of the human brain. It uses artificial neural networks (ANNs) to uncover complex patterns in data and deliver accurate predictions.\n",
    "\n",
    "In this project, a deep learning model is designed to predict the target variables (`size_nm`, `S_abs_nm_Y1`, and `PL`) using the following architecture:\n",
    "\n",
    "1. **Dense Layers**: Fully connected layers with ReLU activation functions to capture non-linear relationships between features.\n",
    "2. **Dropout Layers**: Regularization layers that randomly ignore neurons during training to prevent overfitting.\n",
    "3. **L2 Regularization**: Penalty applied to weights to improve the model’s generalization ability.\n",
    "4. **Adam Optimizer**: An efficient optimization algorithm with an adaptive learning rate for faster convergence.\n",
    "\n",
    "The model is trained using the **Mean Squared Error (MSE)** loss function to minimize the prediction error. Performance is evaluated on both training and testing datasets using the following metrics:\n",
    "- **R² (Coefficient of Determination)**: Indicates how well the model explains the variance in the target variable.\n",
    "- **RMSE (Root Mean Squared Error)**: Measures the average error in predictions.\n",
    "- **MAE (Mean Absolute Error)**: Calculates the average absolute difference between observed and predicted values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Step 1: Load and Preprocess Data\n",
    "def load_and_preprocess_data(file_path, modify=False):\n",
    "    \"\"\"\n",
    "    Load and preprocess the dataset.\n",
    "    \n",
    "    Parameters:\n",
    "        file_path (str): Path to the Excel file.\n",
    "        modify (bool): Whether to perform feature engineering and save as 'modified_data.xlsx'.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Preprocessed dataset.\n",
    "    \"\"\"\n",
    "    data = pd.read_excel(file_path)\n",
    "    \n",
    "    if modify:\n",
    "        # Feature Engineering: Interaction between 'Cl_mmol' and 'Pb_mmol'\n",
    "        data['Cl_Pb_interact'] = data['Cl_mmol'] * data['Pb_mmol']\n",
    "        \n",
    "        # Normalizing skewed features\n",
    "        data['Cl_mmol_log'] = np.log(data['Cl_mmol'] + 1)\n",
    "        \n",
    "        # Remove outliers based on Z-score\n",
    "        z_scores = np.abs(stats.zscore(data.select_dtypes(include=[np.number])))\n",
    "        data = data[(z_scores < 3).all(axis=1)]\n",
    "        \n",
    "        # Scale numerical features\n",
    "        scaler = StandardScaler()\n",
    "        numerical_columns = data.select_dtypes(include=[np.number]).columns\n",
    "        data[numerical_columns] = scaler.fit_transform(data[numerical_columns])\n",
    "        \n",
    "        # Save the modified dataset\n",
    "        data.to_excel(\"modified_data.xlsx\", index=False)\n",
    "        print(\"Modified dataset saved as 'modified_data.xlsx'.\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Load and preprocess the original dataset\n",
    "file_path_original = \"./CsPbCl3_QDs.xlsx\"\n",
    "data_original = load_and_preprocess_data(file_path_original)\n",
    "\n",
    "# Load and preprocess the modified dataset\n",
    "file_path_modified = \"./modified_data.xlsx\"\n",
    "data_modified = load_and_preprocess_data(file_path_modified, modify=False)\n",
    "\n",
    "# Step 2: Prepare Data for Machine Learning\n",
    "def prepare_ml_data(data, target_column):\n",
    "    \"\"\"\n",
    "    Prepare the dataset for machine learning.\n",
    "    \n",
    "    Parameters:\n",
    "        data (pd.DataFrame): Dataset.\n",
    "        target_column (str): Target variable.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple: Features (X), target (y), and train-test splits (X_train, X_test, y_train, y_test).\n",
    "    \"\"\"\n",
    "    # Identify categorical columns\n",
    "    categorical_columns = data.select_dtypes(include=['object']).columns\n",
    "    \n",
    "    # Apply one-hot encoding to categorical columns\n",
    "    one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
    "    one_hot_encoded = one_hot_encoder.fit_transform(data[categorical_columns])\n",
    "    one_hot_encoded_df = pd.DataFrame(\n",
    "        one_hot_encoded, \n",
    "        columns=one_hot_encoder.get_feature_names_out(categorical_columns)\n",
    "    )\n",
    "    \n",
    "    # Replace categorical columns with one-hot encoded columns\n",
    "    data_encoded = data.drop(categorical_columns, axis=1)\n",
    "    data_encoded = pd.concat([data_encoded, one_hot_encoded_df], axis=1)\n",
    "    \n",
    "    # Define features and target\n",
    "    X = data_encoded.drop(target_column, axis=1)\n",
    "    y = data_encoded[target_column]\n",
    "    \n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    return X, y, X_train, X_test, y_train, y_test\n",
    "\n",
    "# Step 3: Define and Train Deep Learning Model\n",
    "def train_deep_learning_model(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Train a deep learning model on the dataset.\n",
    "    \n",
    "    Parameters:\n",
    "        X_train (pd.DataFrame): Training features.\n",
    "        y_train (pd.Series): Training target.\n",
    "        X_test (pd.DataFrame): Testing features.\n",
    "        y_test (pd.Series): Testing target.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Model predictions and performance metrics.\n",
    "    \"\"\"\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train.fillna(X_train.mean()))\n",
    "    X_test_scaled = scaler.transform(X_test.fillna(X_train.mean()))\n",
    "    \n",
    "    # Define the model\n",
    "    model = Sequential([\n",
    "        Dense(64, input_dim=X_train_scaled.shape[1], activation='relu', kernel_regularizer=l2(0.01)),\n",
    "        Dropout(0.5),\n",
    "        Dense(32, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "        Dropout(0.5),\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train_scaled, y_train.fillna(y_train.mean()), epochs=100, batch_size=10, verbose=1, validation_split=0.2)\n",
    "    \n",
    "    # Predictions\n",
    "    predictions_train = model.predict(X_train_scaled).flatten()\n",
    "    predictions_test = model.predict(X_test_scaled).flatten()\n",
    "    \n",
    "    # Compute performance metrics\n",
    "    metrics = {\n",
    "        \"Train R2\": r2_score(y_train.fillna(y_train.mean()), predictions_train),\n",
    "        \"Train RMSE\": np.sqrt(mean_squared_error(y_train.fillna(y_train.mean()), predictions_train)),\n",
    "        \"Train MAE\": mean_absolute_error(y_train.fillna(y_train.mean()), predictions_train),\n",
    "        \"Test R2\": r2_score(y_test, predictions_test),\n",
    "        \"Test RMSE\": np.sqrt(mean_squared_error(y_test, predictions_test)),\n",
    "        \"Test MAE\": mean_absolute_error(y_test, predictions_test)\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        \"predictions_train\": predictions_train,\n",
    "        \"predictions_test\": predictions_test,\n",
    "        \"metrics\": metrics\n",
    "    }\n",
    "\n",
    "# Step 4: Evaluate Targets\n",
    "targets = ['size_nm', 'S_abs_nm_Y1', 'PL']\n",
    "results = {}\n",
    "\n",
    "for target in targets:\n",
    "    print(f\"Evaluating target: {target}\")\n",
    "    \n",
    "    # Prepare data\n",
    "    _, _, X_train, X_test, y_train, y_test = prepare_ml_data(data_modified, target)\n",
    "    \n",
    "    # Train model and get results\n",
    "    results[target] = train_deep_learning_model(X_train, y_train, X_test, y_test)\n",
    "\n",
    "    # Print metrics\n",
    "    print(f\"Metrics for {target}:\")\n",
    "    for metric, value in results[target][\"metrics\"].items():\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Step 5: Visualization\n",
    "fig, axs = plt.subplots(3, 2, figsize=(15, 15))\n",
    "\n",
    "for i, target in enumerate(targets):\n",
    "    y_test = results[target]['predictions_test']\n",
    "    predictions_test = results[target]['predictions_test']\n",
    "\n",
    "    # Plot 1: Observed vs Predicted\n",
    "    sns.scatterplot(x=np.arange(len(y_test)), y=y_test, ax=axs[i, 0], label='Observed', color='red')\n",
    "    sns.scatterplot(x=np.arange(len(predictions_test)), y=predictions_test, ax=axs[i, 0], label='Predicted', color='blue')\n",
    "    axs[i, 0].set_title(f'{target} - Observed vs Predicted')\n",
    "    \n",
    "    # Plot 2: Residuals\n",
    "    residuals = y_test - predictions_test\n",
    "    sns.histplot(residuals, ax=axs[i, 1], kde=True, color='green')\n",
    "    axs[i, 1].set_title(f'{target} - Residuals Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
